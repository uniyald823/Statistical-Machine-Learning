{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5009bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the image\n",
    "img = Image.open('D:/SML_Train/Train_0.jpg')\n",
    "\n",
    "# Get the dimensions of the image\n",
    "width, height = img.size\n",
    "newsize=(100,100)\n",
    "im1 = img.resize(newsize)\n",
    "print(im1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d00feec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import seaborn as sb\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing \n",
    "from PIL import Image\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Conv2D, GlobalAveragePooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn; sn.set(font_scale=1.4)\n",
    "from sklearn.utils import shuffle           \n",
    "import matplotlib.pyplot as plt             \n",
    "import cv2                                 \n",
    "import tensorflow as tf      \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Model \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, AveragePooling2D, Flatten,Conv2D,MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.callbacks import Callback,ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32297585",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (100,100)\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58637c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# def load_data():\n",
    "#     \"\"\"\n",
    "#         Load the data:\n",
    "#             - 14,034 images to train the network.\n",
    "#             - 3,000 images to evaluate how accurately the network learned to classify images.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     dataset = 'D:/SML_Train/'\n",
    "#     labels_df = pd.read_csv('C:/Users/HP/Downloads/SML_Train.csv')\n",
    "#     labels_df['category'] = labels_df['category'].astype(str)\n",
    "#     class_names = sorted(list(set(labels_df['category'])))\n",
    "#     class_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "    \n",
    "#     train_datagen = ImageDataGenerator(\n",
    "#         rescale=1./255,\n",
    "#         rotation_range=20,\n",
    "#         width_shift_range=0.1,\n",
    "#         height_shift_range=0.1,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True,\n",
    "#         fill_mode='nearest'\n",
    "#     )\n",
    "    \n",
    "#     train_generator = train_datagen.flow_from_dataframe(\n",
    "#         dataframe=labels_df,\n",
    "#         directory=dataset,\n",
    "#         x_col='id',\n",
    "#         y_col='category',\n",
    "#         target_size=IMAGE_SIZE,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         class_mode='categorical',\n",
    "#         shuffle=True,\n",
    "#         seed=42\n",
    "#     )\n",
    "    \n",
    "#     return train_generator, class_names_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b5a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_images, train_labels) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d16276",
   "metadata": {},
   "source": [
    "## Loading the Dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acff5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a Pandas dataframe\n",
    "labels_df = pd.read_csv('C:/Users/HP/Downloads/SML_Train.csv')\n",
    "\n",
    "# Create empty lists to store the images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for index, row in labels_df.iterrows():\n",
    "    # Load the image and convert it to a NumPy array\n",
    "    image = Image.open(f'D:/SML_Train/{row[\"id\"]}')\n",
    "    newsize=(100,100)\n",
    "    image = image.resize(newsize)\n",
    "    image_array = np.array(image)\n",
    "    # Append the image and label to their respective lists\n",
    "    images.append(image_array)\n",
    "    labels.append(row['category'])\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "        Load the data:\n",
    "            - 14,034 images to train the network.\n",
    "            - 3,000 images to evaluate how accurately the network learned to classify images.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = 'D:/SML_Train/'\n",
    "    labels_df = pd.read_csv('C:/Users/HP/Downloads/SML_Train.csv')\n",
    "    class_names = sorted(list(set(labels_df['category'])))\n",
    "    class_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "        \n",
    "    print(\"Loading {}\".format(dataset))\n",
    "        \n",
    "    # Iterate through each image in our folder\n",
    "    for file in tqdm(os.listdir(dataset)):\n",
    "                \n",
    "        # Get the path name of the image\n",
    "        img_path = os.path.join(dataset, file)\n",
    "                \n",
    "        # Open and resize the img\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, IMAGE_SIZE) \n",
    "                \n",
    "        # Get the label of the image\n",
    "        label = labels_df.loc[labels_df['id'] == file, 'category'].iloc[0]\n",
    "        label = class_names_label[label]\n",
    "                \n",
    "        # Append the image and its corresponding label to the output\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "                \n",
    "    images = np.array(images, dtype='float32')\n",
    "    labels = np.array(labels, dtype='int32')   \n",
    "    \n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ffc5738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading D:/SML_Train/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 16000/16000 [01:02<00:00, 255.59it/s]\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580dbac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0717ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images, train_labels = shuffle(train_images, train_labels, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc0aa6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 16000\n",
      "Each image is of size: (100, 100)\n"
     ]
    }
   ],
   "source": [
    "n_train = train_labels.shape[0]\n",
    "print (\"Number of training examples: {}\".format(n_train))\n",
    "print (\"Each image is of size: {}\".format(IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e6dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a8d4b2",
   "metadata": {},
   "source": [
    "## Model 1 - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6907799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "100/100 [==============================] - 86s 835ms/step - loss: 2.8938 - accuracy: 0.1795 - recall: 0.0172 - precision: 0.7143 - val_loss: 2.6056 - val_accuracy: 0.2688 - val_recall: 0.0550 - val_precision: 0.7068\n",
      "Epoch 2/25\n",
      "100/100 [==============================] - 82s 821ms/step - loss: 2.3521 - accuracy: 0.3408 - recall: 0.1029 - precision: 0.7264 - val_loss: 2.3988 - val_accuracy: 0.3331 - val_recall: 0.1106 - val_precision: 0.7052\n",
      "Epoch 3/25\n",
      "100/100 [==============================] - 82s 825ms/step - loss: 2.0601 - accuracy: 0.4148 - recall: 0.1789 - precision: 0.7626 - val_loss: 2.3614 - val_accuracy: 0.3512 - val_recall: 0.1391 - val_precision: 0.6773\n",
      "Epoch 4/25\n",
      "100/100 [==============================] - 71s 707ms/step - loss: 1.8037 - accuracy: 0.4877 - recall: 0.2527 - precision: 0.7932 - val_loss: 2.2638 - val_accuracy: 0.3837 - val_recall: 0.1891 - val_precision: 0.6656\n",
      "Epoch 5/25\n",
      "100/100 [==============================] - 163s 2s/step - loss: 1.5284 - accuracy: 0.5592 - recall: 0.3388 - precision: 0.8138 - val_loss: 2.4504 - val_accuracy: 0.3487 - val_recall: 0.1941 - val_precision: 0.5943\n",
      "Epoch 6/25\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 1.2714 - accuracy: 0.6279 - recall: 0.4410 - precision: 0.8519 - val_loss: 2.4937 - val_accuracy: 0.3647 - val_recall: 0.2384 - val_precision: 0.5529\n",
      "Epoch 7/25\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.9724 - accuracy: 0.7235 - recall: 0.5655 - precision: 0.8895 - val_loss: 2.7011 - val_accuracy: 0.3597 - val_recall: 0.2591 - val_precision: 0.5095\n",
      "Epoch 8/25\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.7434 - accuracy: 0.7892 - recall: 0.6689 - precision: 0.9114 - val_loss: 3.0121 - val_accuracy: 0.3369 - val_recall: 0.2650 - val_precision: 0.4599\n",
      "Epoch 9/25\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.5442 - accuracy: 0.8541 - recall: 0.7613 - precision: 0.9393 - val_loss: 3.3345 - val_accuracy: 0.3462 - val_recall: 0.2844 - val_precision: 0.4266\n",
      "Epoch 10/25\n",
      "100/100 [==============================] - 49s 495ms/step - loss: 0.3761 - accuracy: 0.9050 - recall: 0.8441 - precision: 0.9599 - val_loss: 3.5595 - val_accuracy: 0.3388 - val_recall: 0.2909 - val_precision: 0.4127\n",
      "Epoch 11/25\n",
      "100/100 [==============================] - 48s 485ms/step - loss: 0.2509 - accuracy: 0.9404 - recall: 0.9031 - precision: 0.9740 - val_loss: 3.9890 - val_accuracy: 0.3341 - val_recall: 0.2941 - val_precision: 0.3929\n",
      "Epoch 12/25\n",
      "100/100 [==============================] - 48s 486ms/step - loss: 0.1642 - accuracy: 0.9675 - recall: 0.9450 - precision: 0.9848 - val_loss: 4.2847 - val_accuracy: 0.3316 - val_recall: 0.2956 - val_precision: 0.3779\n",
      "Epoch 13/25\n",
      "100/100 [==============================] - 48s 486ms/step - loss: 0.1041 - accuracy: 0.9823 - recall: 0.9702 - precision: 0.9912 - val_loss: 4.5605 - val_accuracy: 0.3244 - val_recall: 0.2962 - val_precision: 0.3664\n",
      "Epoch 14/25\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.0637 - accuracy: 0.9922 - recall: 0.9859 - precision: 0.9961 - val_loss: 4.8291 - val_accuracy: 0.3212 - val_recall: 0.2962 - val_precision: 0.3590\n",
      "Epoch 15/25\n",
      "100/100 [==============================] - 50s 498ms/step - loss: 0.0407 - accuracy: 0.9956 - recall: 0.9927 - precision: 0.9980 - val_loss: 5.1435 - val_accuracy: 0.3303 - val_recall: 0.3059 - val_precision: 0.3559\n",
      "Epoch 16/25\n",
      "100/100 [==============================] - 50s 500ms/step - loss: 0.0253 - accuracy: 0.9985 - recall: 0.9964 - precision: 0.9991 - val_loss: 5.3400 - val_accuracy: 0.3313 - val_recall: 0.3103 - val_precision: 0.3576\n",
      "Epoch 17/25\n",
      "100/100 [==============================] - 49s 491ms/step - loss: 0.0181 - accuracy: 0.9987 - recall: 0.9980 - precision: 0.9991 - val_loss: 5.5297 - val_accuracy: 0.3253 - val_recall: 0.3072 - val_precision: 0.3488\n",
      "Epoch 18/25\n",
      "100/100 [==============================] - 50s 502ms/step - loss: 0.0133 - accuracy: 0.9994 - recall: 0.9989 - precision: 0.9995 - val_loss: 5.6520 - val_accuracy: 0.3338 - val_recall: 0.3156 - val_precision: 0.3579\n",
      "Epoch 19/25\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.0100 - accuracy: 0.9993 - recall: 0.9991 - precision: 0.9995 - val_loss: 5.8976 - val_accuracy: 0.3322 - val_recall: 0.3144 - val_precision: 0.3514\n",
      "Epoch 20/25\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0758 - accuracy: 0.9797 - recall: 0.9762 - precision: 0.9832 - val_loss: 5.8025 - val_accuracy: 0.3069 - val_recall: 0.2891 - val_precision: 0.3321\n",
      "Epoch 21/25\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.2851 - accuracy: 0.9091 - recall: 0.8868 - precision: 0.9281 - val_loss: 5.5009 - val_accuracy: 0.3200 - val_recall: 0.3013 - val_precision: 0.3513\n",
      "Epoch 22/25\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.1530 - accuracy: 0.9581 - recall: 0.9453 - precision: 0.9682 - val_loss: 5.4230 - val_accuracy: 0.3197 - val_recall: 0.3006 - val_precision: 0.3524\n",
      "Epoch 23/25\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0625 - accuracy: 0.9871 - recall: 0.9824 - precision: 0.9903 - val_loss: 5.7511 - val_accuracy: 0.3309 - val_recall: 0.3156 - val_precision: 0.3556\n",
      "Epoch 24/25\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0255 - accuracy: 0.9957 - recall: 0.9950 - precision: 0.9966 - val_loss: 6.0911 - val_accuracy: 0.3313 - val_recall: 0.3178 - val_precision: 0.3495\n",
      "Epoch 25/25\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0109 - accuracy: 0.9991 - recall: 0.9991 - precision: 0.9992 - val_loss: 6.2096 - val_accuracy: 0.3300 - val_recall: 0.3134 - val_precision: 0.3474\n"
     ]
    }
   ],
   "source": [
    "train_labels_new = to_categorical(train_labels)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (100,100,3)), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(25, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=[\"accuracy\",tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "history = model.fit(train_images, train_labels_new, batch_size=128, epochs=25, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4f113c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    \"\"\"\n",
    "        Plot the accuracy and the loss during the training of the nn.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history.history['accuracy'],'bo--', label = \"acc\")\n",
    "    plt.plot(history.history['val_accuracy'], 'ro--', label = \"val_acc\")\n",
    "    plt.title(\"train_acc vs val_acc\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss function\n",
    "    plt.subplot(222)\n",
    "    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n",
    "    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n",
    "    plt.title(\"train_loss vs val_loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ad7d4db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5564bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# # load the model\n",
    "# model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db292b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the test images\n",
    "test_path = 'D:/sml_kaggle2/SML_Test/'\n",
    "\n",
    "# Get a list of all image file names in the directory\n",
    "image_files = os.listdir(test_path)\n",
    "\n",
    "# Sort the list of image file names in ascending order\n",
    "image_files.sort(key=lambda x: int(x.split('.')[0].split('_')[1]))\n",
    "\n",
    "# Load the test images into a NumPy array\n",
    "test_images = []\n",
    "for file in image_files:\n",
    "    image = Image.open(os.path.join(test_path, file))\n",
    "    newsize=(100,100)\n",
    "    image = image.resize(newsize)\n",
    "    image_array = np.array(image)\n",
    "    test_images.append(image_array)\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Predict the labels of the test images using the trained model\n",
    "predictions = model.predict(test_images)\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame with image ids and predicted labels\n",
    "image_ids = [file for file in image_files]\n",
    "results_df = pd.DataFrame({'image_id': image_ids, 'label': pred_labels})\n",
    "\n",
    "# Save results to CSV file\n",
    "results_df.to_csv('mt21119_CNN.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33061d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path to the test images\n",
    "# test_path = 'D:/sml_kaggle2/SML_Test/'\n",
    "\n",
    "# # Get a list of all image file names in the directory\n",
    "# image_files = os.listdir(test_path)\n",
    "\n",
    "# # Sort the list of image file names in ascending order\n",
    "# image_files.sort(key=lambda x: int(x.split('.')[0].split('_')[1]))\n",
    "\n",
    "# # Load the test images into a NumPy array\n",
    "# test_images = []\n",
    "# for file in image_files:\n",
    "#     image = Image.open(os.path.join(test_path, file))\n",
    "#     image_array = np.array(image)\n",
    "#     test_images.append(image_array)\n",
    "# test_images = np.array(test_images)\n",
    "\n",
    "# # Normalize the pixel values to be between 0 and 1\n",
    "# test_images = test_images / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4117b682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2442880e",
   "metadata": {},
   "source": [
    "## Model 2 VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e71e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 438s 876ms/step\n",
      "47/47 [==============================] - 42s 887ms/step\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "train_features = model.predict(train_images)\n",
    "test_features = model.predict(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "206c82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, x, y, z = train_features.shape\n",
    "numFeatures = x * y * z\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components = 2)\n",
    "X = train_features.reshape((n_train, x*y*z))\n",
    "pca.fit(X)\n",
    "C = pca.transform(X) # Représentation des individus dans les nouveaux axe\n",
    "C1 = C[:,0]\n",
    "C2 = C[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a167b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "500/500 [==============================] - 2s 2ms/step - loss: 3.0158 - accuracy: 0.1542 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.6820 - accuracy: 0.2939 - recall_1: 0.0031 - precision_1: 1.0000\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.4639 - accuracy: 0.3539 - recall_1: 0.0222 - precision_1: 0.9861\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.2980 - accuracy: 0.3929 - recall_1: 0.0456 - precision_1: 0.9720\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.1814 - accuracy: 0.4181 - recall_1: 0.0680 - precision_1: 0.9544\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.0958 - accuracy: 0.4394 - recall_1: 0.0923 - precision_1: 0.9426\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.0290 - accuracy: 0.4543 - recall_1: 0.1115 - precision_1: 0.9326\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.9735 - accuracy: 0.4650 - recall_1: 0.1308 - precision_1: 0.9277\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.9257 - accuracy: 0.4761 - recall_1: 0.1479 - precision_1: 0.9221\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.8850 - accuracy: 0.4878 - recall_1: 0.1621 - precision_1: 0.9189\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.8496 - accuracy: 0.4972 - recall_1: 0.1754 - precision_1: 0.9108\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.8184 - accuracy: 0.5024 - recall_1: 0.1882 - precision_1: 0.9077\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.7909 - accuracy: 0.5092 - recall_1: 0.1979 - precision_1: 0.9015\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.7660 - accuracy: 0.5154 - recall_1: 0.2102 - precision_1: 0.9014\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.7431 - accuracy: 0.5177 - recall_1: 0.2183 - precision_1: 0.8975\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.7224 - accuracy: 0.5241 - recall_1: 0.2272 - precision_1: 0.8966\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.7032 - accuracy: 0.5286 - recall_1: 0.2357 - precision_1: 0.8964\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.6861 - accuracy: 0.5326 - recall_1: 0.2428 - precision_1: 0.8921\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.6700 - accuracy: 0.5356 - recall_1: 0.2494 - precision_1: 0.8896\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.6552 - accuracy: 0.5398 - recall_1: 0.2562 - precision_1: 0.8907\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.6413 - accuracy: 0.5431 - recall_1: 0.2618 - precision_1: 0.8899\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.6283 - accuracy: 0.5449 - recall_1: 0.2686 - precision_1: 0.8876\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.6161 - accuracy: 0.5491 - recall_1: 0.2729 - precision_1: 0.8881\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.6047 - accuracy: 0.5509 - recall_1: 0.2774 - precision_1: 0.8862\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.5935 - accuracy: 0.5532 - recall_1: 0.2812 - precision_1: 0.8874\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (x, y, z)),\n",
    "    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(25, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model2.compile(optimizer = 'adagrad', loss = 'categorical_crossentropy', metrics=[\"accuracy\",tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "\n",
    "history2 = model2.fit(train_features,train_labels_new,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a917f70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save('D:/vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd0c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94ca0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path to the test images\n",
    "# test_path = 'D:/sml_kaggle2/SML_Test/'\n",
    "\n",
    "# # Get a list of all image file names in the directory\n",
    "# image_files = os.listdir(test_path)\n",
    "\n",
    "# # Sort the list of image file names in ascending order\n",
    "# image_files.sort(key=lambda x: int(x.split('.')[0].split('_')[1]))\n",
    "\n",
    "# # Load the test images into a NumPy array\n",
    "# test_images = []\n",
    "# for file in image_files:\n",
    "#     image = Image.open(os.path.join(test_path, file))\n",
    "#     newsize=(100,100)\n",
    "#     image = image.resize(newsize)\n",
    "#     image_array = np.array(image)\n",
    "#     test_images.append(image_array)\n",
    "# test_images = np.array(test_images)\n",
    "\n",
    "# # Normalize the pixel values to be between 0 and 1\n",
    "# test_images = test_images / 255.0\n",
    "\n",
    "# # Predict the labels of the test images using the trained model\n",
    "# predictions = model2.predict(test_images)\n",
    "# pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# # Create DataFrame with image ids and predicted labels\n",
    "# image_ids = [file for file in image_files]\n",
    "# results_df = pd.DataFrame({'image_id': image_ids, 'label': pred_labels})\n",
    "\n",
    "# # Save results to CSV file\n",
    "# #results_df.to_csv('mt21119_CNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884d011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79252f5c",
   "metadata": {},
   "source": [
    "## Model 3 VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4df1d572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "500/500 [==============================] - 556s 1s/step - loss: 2.2023 - accuracy: 0.3568 - recall_4: 0.1598 - precision_4: 0.7144\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 568s 1s/step - loss: 1.7931 - accuracy: 0.4647 - recall_4: 0.2862 - precision_4: 0.7433\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 557s 1s/step - loss: 1.6563 - accuracy: 0.5081 - recall_4: 0.3386 - precision_4: 0.7603\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 531s 1s/step - loss: 1.5495 - accuracy: 0.5357 - recall_4: 0.3766 - precision_4: 0.7683\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 541s 1s/step - loss: 1.4514 - accuracy: 0.5641 - recall_4: 0.4155 - precision_4: 0.7808\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 526s 1s/step - loss: 1.3613 - accuracy: 0.5888 - recall_4: 0.4446 - precision_4: 0.7921\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 501s 1s/step - loss: 1.2650 - accuracy: 0.6165 - recall_4: 0.4816 - precision_4: 0.8041\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 510s 1s/step - loss: 1.1799 - accuracy: 0.6329 - recall_4: 0.5073 - precision_4: 0.8105\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 546s 1s/step - loss: 1.0854 - accuracy: 0.6628 - recall_4: 0.5434 - precision_4: 0.8225\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 573s 1s/step - loss: 0.9865 - accuracy: 0.6871 - recall_4: 0.5826 - precision_4: 0.8345\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 521s 1s/step - loss: 0.8861 - accuracy: 0.7222 - recall_4: 0.6198 - precision_4: 0.8478\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 510s 1s/step - loss: 0.8041 - accuracy: 0.7412 - recall_4: 0.6529 - precision_4: 0.8552\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 551s 1s/step - loss: 0.6864 - accuracy: 0.7804 - recall_4: 0.7028 - precision_4: 0.8758\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 534s 1s/step - loss: 0.6176 - accuracy: 0.7991 - recall_4: 0.7321 - precision_4: 0.8815\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 511s 1s/step - loss: 0.5258 - accuracy: 0.8301 - recall_4: 0.7729 - precision_4: 0.8957\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 511s 1s/step - loss: 0.4417 - accuracy: 0.8517 - recall_4: 0.8057 - precision_4: 0.9053\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 511s 1s/step - loss: 0.4015 - accuracy: 0.8668 - recall_4: 0.8273 - precision_4: 0.9131\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 512s 1s/step - loss: 0.3313 - accuracy: 0.8907 - recall_4: 0.8592 - precision_4: 0.9265\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 510s 1s/step - loss: 0.2955 - accuracy: 0.9011 - recall_4: 0.8741 - precision_4: 0.9302\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 511s 1s/step - loss: 0.2505 - accuracy: 0.9185 - recall_4: 0.8963 - precision_4: 0.9404\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 512s 1s/step - loss: 0.2401 - accuracy: 0.9201 - recall_4: 0.8999 - precision_4: 0.9409\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 511s 1s/step - loss: 0.1925 - accuracy: 0.9358 - recall_4: 0.9214 - precision_4: 0.9503\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 514s 1s/step - loss: 0.2104 - accuracy: 0.9291 - recall_4: 0.9136 - precision_4: 0.9450\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 543s 1s/step - loss: 0.1725 - accuracy: 0.9452 - recall_4: 0.9348 - precision_4: 0.9550\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 543s 1s/step - loss: 0.1794 - accuracy: 0.9390 - recall_4: 0.9294 - precision_4: 0.9505\n"
     ]
    }
   ],
   "source": [
    "vgg19_model = VGG19(weights='imagenet',include_top=False)\n",
    "x=vgg19_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "preds=Dense(25, activation='softmax')(x) #final layer with softmax activation\n",
    "\n",
    "newModel=Model(inputs=vgg19_model.input,outputs=preds)\n",
    "for layer in newModel.layers[:-5]:\n",
    "    layer.trainable=False\n",
    "    \n",
    "newModel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[\"accuracy\",tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "history = newModel.fit(train_images,train_labels_new,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4de23964",
   "metadata": {},
   "outputs": [],
   "source": [
    "newModel.save('D:/vgg19/.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ecb57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2faa5d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 23s 491ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the test images\n",
    "test_path = 'D:/sml_kaggle2/SML_Test/'\n",
    "\n",
    "# Get a list of all image file names in the directory\n",
    "image_files = os.listdir(test_path)\n",
    "\n",
    "# Sort the list of image file names in ascending order\n",
    "image_files.sort(key=lambda x: int(x.split('.')[0].split('_')[1]))\n",
    "\n",
    "# Load the test images into a NumPy array\n",
    "test_images = []\n",
    "for file in image_files:\n",
    "    image = Image.open(os.path.join(test_path, file))\n",
    "    image_array = np.array(image)\n",
    "    test_images.append(image_array)\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Predict the labels of the test images using the trained model\n",
    "predictions = newModel.predict(test_images)\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame with image ids and predicted labels\n",
    "image_ids = [file for file in image_files]\n",
    "results_df = pd.DataFrame({'id': image_ids, 'category': pred_labels})\n",
    "\n",
    "# Save results to CSV file\n",
    "results_df.to_csv('D:/vgg19.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47026a7c",
   "metadata": {},
   "source": [
    "## Model 4 ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d776713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.2144 - accuracy: 0.0584 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 258s 502ms/step - loss: 3.2144 - accuracy: 0.0584 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.1660 - accuracy: 0.0634 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 254s 508ms/step - loss: 3.1660 - accuracy: 0.0634 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.1464 - accuracy: 0.0677 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 255s 511ms/step - loss: 3.1464 - accuracy: 0.0677 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.1320 - accuracy: 0.0709 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 251s 502ms/step - loss: 3.1320 - accuracy: 0.0709 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.1167 - accuracy: 0.0779 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 247s 494ms/step - loss: 3.1167 - accuracy: 0.0779 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.1117 - accuracy: 0.0790 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 239s 479ms/step - loss: 3.1117 - accuracy: 0.0790 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.1103 - accuracy: 0.0822 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 239s 477ms/step - loss: 3.1103 - accuracy: 0.0822 - recall_5: 0.0000e+00 - precision_5: 0.0000e+00\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.1054 - accuracy: 0.0845 - recall_5: 1.8750e-04 - precision_5: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 240s 479ms/step - loss: 3.1054 - accuracy: 0.0845 - recall_5: 1.8750e-04 - precision_5: 1.0000\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.1023 - accuracy: 0.0837 - recall_5: 1.2500e-04 - precision_5: 0.4000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 241s 481ms/step - loss: 3.1023 - accuracy: 0.0837 - recall_5: 1.2500e-04 - precision_5: 0.4000\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0967 - accuracy: 0.0911 - recall_5: 7.5000e-04 - precision_5: 0.8571WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 241s 483ms/step - loss: 3.0967 - accuracy: 0.0911 - recall_5: 7.5000e-04 - precision_5: 0.8571\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.1000 - accuracy: 0.0844 - recall_5: 3.1250e-04 - precision_5: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 320s 640ms/step - loss: 3.1000 - accuracy: 0.0844 - recall_5: 3.1250e-04 - precision_5: 0.8333\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0958 - accuracy: 0.0878 - recall_5: 4.3750e-04 - precision_5: 0.7000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 209s 419ms/step - loss: 3.0958 - accuracy: 0.0878 - recall_5: 4.3750e-04 - precision_5: 0.7000\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0943 - accuracy: 0.0877 - recall_5: 4.3750e-04 - precision_5: 0.7000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 217s 433ms/step - loss: 3.0943 - accuracy: 0.0877 - recall_5: 4.3750e-04 - precision_5: 0.7000\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0951 - accuracy: 0.0914 - recall_5: 6.2500e-04 - precision_5: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 218s 436ms/step - loss: 3.0951 - accuracy: 0.0914 - recall_5: 6.2500e-04 - precision_5: 0.6667\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0956 - accuracy: 0.0891 - recall_5: 8.1250e-04 - precision_5: 0.7222WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 232s 464ms/step - loss: 3.0956 - accuracy: 0.0891 - recall_5: 8.1250e-04 - precision_5: 0.7222\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0938 - accuracy: 0.0916 - recall_5: 9.3750e-04 - precision_5: 0.6818WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 3.0938 - accuracy: 0.0916 - recall_5: 9.3750e-04 - precision_5: 0.6818\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0900 - accuracy: 0.0928 - recall_5: 7.5000e-04 - precision_5: 0.6000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 220s 440ms/step - loss: 3.0900 - accuracy: 0.0928 - recall_5: 7.5000e-04 - precision_5: 0.6000\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0861 - accuracy: 0.0937 - recall_5: 9.3750e-04 - precision_5: 0.7143WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 220s 440ms/step - loss: 3.0861 - accuracy: 0.0937 - recall_5: 9.3750e-04 - precision_5: 0.7143\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0916 - accuracy: 0.0941 - recall_5: 8.1250e-04 - precision_5: 0.6190WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 221s 443ms/step - loss: 3.0916 - accuracy: 0.0941 - recall_5: 8.1250e-04 - precision_5: 0.6190\n",
      "Epoch 20/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - ETA: 0s - loss: 3.0877 - accuracy: 0.0920 - recall_5: 6.2500e-04 - precision_5: 0.4167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 218s 436ms/step - loss: 3.0877 - accuracy: 0.0920 - recall_5: 6.2500e-04 - precision_5: 0.4167\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0882 - accuracy: 0.0938 - recall_5: 4.3750e-04 - precision_5: 0.6364WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 3.0882 - accuracy: 0.0938 - recall_5: 4.3750e-04 - precision_5: 0.6364\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0871 - accuracy: 0.0927 - recall_5: 0.0015 - precision_5: 0.6316WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 221s 441ms/step - loss: 3.0871 - accuracy: 0.0927 - recall_5: 0.0015 - precision_5: 0.6316\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0875 - accuracy: 0.0926 - recall_5: 9.3750e-04 - precision_5: 0.6250WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 221s 442ms/step - loss: 3.0875 - accuracy: 0.0926 - recall_5: 9.3750e-04 - precision_5: 0.6250\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0807 - accuracy: 0.0922 - recall_5: 3.7500e-04 - precision_5: 0.7500WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 219s 437ms/step - loss: 3.0807 - accuracy: 0.0922 - recall_5: 3.7500e-04 - precision_5: 0.7500\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.0856 - accuracy: 0.0919 - recall_5: 2.5000e-04 - precision_5: 0.3636WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,recall_5,precision_5\n",
      "500/500 [==============================] - 218s 435ms/step - loss: 3.0856 - accuracy: 0.0919 - recall_5: 2.5000e-04 - precision_5: 0.3636\n"
     ]
    }
   ],
   "source": [
    "#build the Resnet model \n",
    "resnet = ResNet50(weights='imagenet',\n",
    "                      input_shape= (100,100,3),\n",
    "                      include_top= False)  \n",
    "\n",
    "\n",
    "#show how manay layers in the Resnet Network\n",
    "layers = resnet.layers\n",
    "\n",
    "# early stopping \n",
    "callbacks = EarlyStopping(patience = 3, monitor='val_acc')\n",
    "                        \n",
    "# let's train our Model \n",
    "inputs = resnet.input\n",
    "# add an average pooling layer\n",
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "#first dense layer\n",
    "x = Dense(512, activation='relu')(x)\n",
    "#dropout \n",
    "x = Dropout(0.5)(x)\n",
    "# output layer\n",
    "outputs = Dense(25, activation ='softmax')(x)\n",
    "# this is the model we will train\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# freeze all convolutional Resnet layers\n",
    "for layer in layers:\n",
    "    layer.trainable = False\n",
    "# compile the model \n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[\"accuracy\",tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_images,train_labels_new,epochs= 25,\n",
    "    callbacks = [callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "143f29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:/resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "182ce7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "# # load pre-trained InceptionV3\n",
    "# pre_trained = InceptionV3(weights='imagenet', input_shape=(64,64,3), pooling='avg')\n",
    "# train_features = pre_trained.predict(train_images)\n",
    "# test_features = pre_trained.predict(test_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6cdcb",
   "metadata": {},
   "source": [
    "## Model 5 Mobilenetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3316b4ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "[INFO] training ...\n",
      "Epoch 1/25\n",
      "500/500 [==============================] - 92s 176ms/step - loss: 2.7529 - accuracy: 0.2450 - recall_3: 0.1299 - precision_3: 0.7700\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 89s 178ms/step - loss: 2.2059 - accuracy: 0.3950 - recall_3: 0.2692 - precision_3: 0.8179\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 89s 178ms/step - loss: 1.9151 - accuracy: 0.4692 - recall_3: 0.3446 - precision_3: 0.8409\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 92s 184ms/step - loss: 1.6859 - accuracy: 0.5292 - recall_3: 0.3992 - precision_3: 0.8544\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 90s 181ms/step - loss: 1.5281 - accuracy: 0.5675 - recall_3: 0.4420 - precision_3: 0.8604\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 91s 183ms/step - loss: 1.3867 - accuracy: 0.6108 - recall_3: 0.4883 - precision_3: 0.8721\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 88s 176ms/step - loss: 1.2755 - accuracy: 0.6411 - recall_3: 0.5178 - precision_3: 0.8729\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 93s 187ms/step - loss: 1.1942 - accuracy: 0.6610 - recall_3: 0.5460 - precision_3: 0.8800\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 91s 182ms/step - loss: 1.0926 - accuracy: 0.6919 - recall_3: 0.5794 - precision_3: 0.8842\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 91s 183ms/step - loss: 1.0156 - accuracy: 0.7144 - recall_3: 0.6086 - precision_3: 0.8949\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 89s 179ms/step - loss: 0.9379 - accuracy: 0.7369 - recall_3: 0.6401 - precision_3: 0.8979\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 88s 176ms/step - loss: 0.8821 - accuracy: 0.7540 - recall_3: 0.6591 - precision_3: 0.9017\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 89s 177ms/step - loss: 0.8166 - accuracy: 0.7682 - recall_3: 0.6839 - precision_3: 0.9055\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 75s 151ms/step - loss: 0.7784 - accuracy: 0.7807 - recall_3: 0.6999 - precision_3: 0.9063\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 78s 156ms/step - loss: 0.7166 - accuracy: 0.7909 - recall_3: 0.7182 - precision_3: 0.9116\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 83s 166ms/step - loss: 0.6573 - accuracy: 0.8115 - recall_3: 0.7487 - precision_3: 0.9186\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 82s 163ms/step - loss: 0.6208 - accuracy: 0.8220 - recall_3: 0.7581 - precision_3: 0.9242\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 78s 155ms/step - loss: 0.6010 - accuracy: 0.8338 - recall_3: 0.7721 - precision_3: 0.9250\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 81s 161ms/step - loss: 0.5729 - accuracy: 0.8369 - recall_3: 0.7803 - precision_3: 0.9255\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 79s 157ms/step - loss: 0.5339 - accuracy: 0.8503 - recall_3: 0.7956 - precision_3: 0.9299\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 80s 160ms/step - loss: 0.5109 - accuracy: 0.8602 - recall_3: 0.8109 - precision_3: 0.9330\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 79s 158ms/step - loss: 0.4972 - accuracy: 0.8614 - recall_3: 0.8146 - precision_3: 0.9343\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 80s 159ms/step - loss: 0.4702 - accuracy: 0.8664 - recall_3: 0.8234 - precision_3: 0.9351\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 79s 157ms/step - loss: 0.4621 - accuracy: 0.8733 - recall_3: 0.8320 - precision_3: 0.9383\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 76s 153ms/step - loss: 0.4330 - accuracy: 0.8789 - recall_3: 0.8383 - precision_3: 0.9397\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def MobileNetV2_model(learning_rate, input_shape):\n",
    "    baseModel = MobileNetV2(include_top=False, input_tensor=Input(shape=(100,100,3)))\n",
    "    for layer in baseModel.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(baseModel)\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(25, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "one_hot_label = to_categorical(train_labels)\n",
    "\n",
    "bs = 32\n",
    "lr = 0.0001\n",
    "shape  =(64,64, 3)\n",
    "size = (128, 128)\n",
    "model = MobileNetV2_model(lr,shape)\n",
    "model.compile(loss= \"categorical_crossentropy\", metrics=[\"accuracy\",tf.keras.metrics.Recall(),tf.keras.metrics.Precision()], optimizer=\"adam\")\n",
    "print(\"[INFO] training ...\")\n",
    "history = model.fit(train_images,one_hot_label,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9472478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 7s 118ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the test images\n",
    "test_path = 'D:/sml_kaggle2/SML_Test/'\n",
    "\n",
    "# Get a list of all image file names in the directory\n",
    "image_files = os.listdir(test_path)\n",
    "\n",
    "# Sort the list of image file names in ascending order\n",
    "image_files.sort(key=lambda x: int(x.split('.')[0].split('_')[1]))\n",
    "\n",
    "# Load the test images into a NumPy array\n",
    "test_images = []\n",
    "for file in image_files:\n",
    "    image = Image.open(os.path.join(test_path, file))\n",
    "    newsize=(100,100)\n",
    "    image = image.resize(newsize)\n",
    "    image_array = np.array(image)\n",
    "    test_images.append(image_array)\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Predict the labels of the test images using the trained model\n",
    "predictions = model.predict(test_images)\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame with image ids and predicted labels\n",
    "image_ids = [file for file in image_files]\n",
    "results_df = pd.DataFrame({'id': image_ids, 'category': pred_labels})\n",
    "\n",
    "# Save results to CSV file\n",
    "results_df.to_csv('D:/mt21119_mobilenet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57369e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "[INFO] training ...\n",
      "Epoch 1/25\n",
      "500/500 [==============================] - 74s 143ms/step - loss: 2.6807 - accuracy: 0.2494 - recall_7: 0.1449 - precision_7: 0.7722\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 73s 145ms/step - loss: 2.1488 - accuracy: 0.4059 - recall_7: 0.2860 - precision_7: 0.8302\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 1.8501 - accuracy: 0.4867 - recall_7: 0.3531 - precision_7: 0.8320\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 1.6580 - accuracy: 0.5433 - recall_7: 0.4054 - precision_7: 0.8470\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 73s 147ms/step - loss: 1.4780 - accuracy: 0.5834 - recall_7: 0.4532 - precision_7: 0.8492\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 74s 147ms/step - loss: 1.3228 - accuracy: 0.6271 - recall_7: 0.4991 - precision_7: 0.8668\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 73s 147ms/step - loss: 1.2267 - accuracy: 0.6513 - recall_7: 0.5324 - precision_7: 0.8713\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 73s 147ms/step - loss: 1.1317 - accuracy: 0.6837 - recall_7: 0.5689 - precision_7: 0.8791\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 74s 147ms/step - loss: 1.0511 - accuracy: 0.7049 - recall_7: 0.5928 - precision_7: 0.8851\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 74s 147ms/step - loss: 0.9537 - accuracy: 0.7324 - recall_7: 0.6313 - precision_7: 0.8963\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 73s 147ms/step - loss: 0.8982 - accuracy: 0.7464 - recall_7: 0.6526 - precision_7: 0.8978\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 74s 147ms/step - loss: 0.8372 - accuracy: 0.7639 - recall_7: 0.6773 - precision_7: 0.9000\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 74s 147ms/step - loss: 0.7782 - accuracy: 0.7798 - recall_7: 0.6984 - precision_7: 0.9065\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 74s 148ms/step - loss: 0.7275 - accuracy: 0.7969 - recall_7: 0.7220 - precision_7: 0.9167\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 74s 148ms/step - loss: 0.6716 - accuracy: 0.8095 - recall_7: 0.7395 - precision_7: 0.9189\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 74s 147ms/step - loss: 0.6595 - accuracy: 0.8189 - recall_7: 0.7536 - precision_7: 0.9195\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 74s 148ms/step - loss: 0.6269 - accuracy: 0.8269 - recall_7: 0.7652 - precision_7: 0.9221\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 74s 148ms/step - loss: 0.5736 - accuracy: 0.8429 - recall_7: 0.7867 - precision_7: 0.9282\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 74s 148ms/step - loss: 0.5451 - accuracy: 0.8497 - recall_7: 0.7972 - precision_7: 0.9292\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 78s 155ms/step - loss: 0.5180 - accuracy: 0.8531 - recall_7: 0.8039 - precision_7: 0.9283\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 77s 154ms/step - loss: 0.5132 - accuracy: 0.8589 - recall_7: 0.8144 - precision_7: 0.9358\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 74s 148ms/step - loss: 0.4807 - accuracy: 0.8694 - recall_7: 0.8247 - precision_7: 0.9400\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 74s 148ms/step - loss: 0.4682 - accuracy: 0.8724 - recall_7: 0.8291 - precision_7: 0.9387\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 75s 150ms/step - loss: 0.4412 - accuracy: 0.8785 - recall_7: 0.8403 - precision_7: 0.9405\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 74s 149ms/step - loss: 0.4292 - accuracy: 0.8815 - recall_7: 0.8447 - precision_7: 0.9427\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def MobileNetV2_model(learning_rate, input_shape):\n",
    "    baseModel = MobileNetV2(include_top=False, input_tensor=Input(shape=(100,100,3)))\n",
    "    for layer in baseModel.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(baseModel)\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(25, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "one_hot_label = to_categorical(train_labels)\n",
    "\n",
    "bs = 32\n",
    "lr = 0.001\n",
    "shape  =(64,64, 3)\n",
    "size = (128, 128)\n",
    "model = MobileNetV2_model(lr,shape)\n",
    "model.compile(loss= \"categorical_crossentropy\", metrics=[\"accuracy\",tf.keras.metrics.Recall(),tf.keras.metrics.Precision()], optimizer=\"adam\")\n",
    "print(\"[INFO] training ...\")\n",
    "history = model.fit(train_images,one_hot_label,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf941c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:/mobilenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fec18e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 9s 164ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the test images\n",
    "test_path = 'D:/sml_kaggle2/SML_Test/'\n",
    "\n",
    "# Get a list of all image file names in the directory\n",
    "image_files = os.listdir(test_path)\n",
    "\n",
    "# Sort the list of image file names in ascending order\n",
    "image_files.sort(key=lambda x: int(x.split('.')[0].split('_')[1]))\n",
    "\n",
    "# Load the test images into a NumPy array\n",
    "test_images = []\n",
    "for file in image_files:\n",
    "    image = Image.open(os.path.join(test_path, file))\n",
    "    newsize=(100,100)\n",
    "    image = image.resize(newsize)\n",
    "    image_array = np.array(image)\n",
    "    test_images.append(image_array)\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Predict the labels of the test images using the trained model\n",
    "predictions = model.predict(test_images)\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame with image ids and predicted labels\n",
    "image_ids = [file for file in image_files]\n",
    "results_df = pd.DataFrame({'id': image_ids, 'category': pred_labels})\n",
    "\n",
    "# Save results to CSV file\n",
    "results_df.to_csv('D:/mt21119_mobilenet2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39df8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
